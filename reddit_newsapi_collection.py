# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G57kwu__FTPhLjDjy9mVejMF_jXgq2lZ
"""

!pip install praw pandas requests

import praw

reddit = praw.Reddit(
    client_id="qgilJPKcQtYxGEoFRfkMTA",
    client_secret="UPxsnS_u1HRbc6O4fZVUET_jTfLr0A",
    user_agent="narrativenexus by u/yourusername"
)

print("Reddit connected:", reddit.read_only)

import pandas as pd
from datetime import datetime

posts = []

# ðŸ‘‡ Add as many subreddits as you want
subreddits = [
    "datascience", "machinelearning", "artificial", "technology",
    "python", "bigdata", "computervision", "deeplearning",
    "programming", "AIethics"
]

for sub in subreddits:
    subreddit = reddit.subreddit(sub)

    # fetch from hot, new, and top
    for post in subreddit.hot(limit=100):
        posts.append({
            "id": post.id,
            "source": "reddit",
            "subreddit": sub,
            "title": post.title,
            "author": str(post.author),
            "text": post.selftext,
            "score": post.score,
            "created": datetime.fromtimestamp(post.created_utc),
            "url": f"https://www.reddit.com{post.permalink}"
        })
    for post in subreddit.new(limit=100):
        posts.append({
            "id": post.id,
            "source": "reddit",
            "subreddit": sub,
            "title": post.title,
            "author": str(post.author),
            "text": post.selftext,
            "score": post.score,
            "created": datetime.fromtimestamp(post.created_utc),
            "url": f"https://www.reddit.com{post.permalink}"
        })
    for post in subreddit.top(limit=100):
        posts.append({
            "id": post.id,
            "source": "reddit",
            "subreddit": sub,
            "title": post.title,
            "author": str(post.author),
            "text": post.selftext,
            "score": post.score,
            "created": datetime.fromtimestamp(post.created_utc),
            "url": f"https://www.reddit.com{post.permalink}"
        })

# Convert into DataFrame
df_reddit = pd.DataFrame(posts)

# Save to CSV
df_reddit.to_csv("reddit_posts.csv", index=False, encoding="utf-8")

print("Reddit rows collected:", len(df_reddit))
df_reddit.head()

len(df_reddit)

import requests

API_KEY = "79482648b96b4d07861ce9e30558b391"
all_articles = []

# keywords you want
keywords = ["AI", "machine learning", "technology", "python", "big data"]

for query in keywords:
    url = f"https://newsapi.org/v2/everything?q={query}&language=en&pageSize=100&apiKey={API_KEY}"
    response = requests.get(url).json()
    if "articles" in response:
        for article in response["articles"]:
            all_articles.append({
                "source": "newsapi",
                "query": query,
                "title": article["title"],
                "author": article["author"],
                "description": article["description"],
                "content": article["content"],
                "publishedAt": article["publishedAt"],
                "url": article["url"]
            })

df_news = pd.DataFrame(all_articles)

print("News rows collected:", len(df_news))
df_news.head()

!pip install git+https://github.com/twintproject/twint.git

!pip install async-timeout

import requests, pandas as pd
from datetime import datetime, timezone

top_ids = requests.get("https://hacker-news.firebaseio.com/v0/topstories.json").json()[:1000]

rows = []
for _id in top_ids:
    item = requests.get(f"https://hacker-news.firebaseio.com/v0/item/{_id}.json").json()
    if item and item.get("type") == "story":
        rows.append({
            "source": "hackernews",
            "id": item["id"],
            "title": item.get("title"),
            "author": item.get("by"),
            "time": datetime.fromtimestamp(item["time"], tz=timezone.utc),
            "url": item.get("url")
        })

df_hn = pd.DataFrame(rows)
print("Hacker News rows:", len(df_hn))
df_hn.to_csv("hackernews_posts.csv", index=False)

!pip install feedparser
import feedparser, pandas as pd

feed = feedparser.parse("https://medium.com/feed/tag/artificial-intelligence")

medium_posts = []
for entry in feed.entries:
    medium_posts.append({
        "source": "medium",
        "title": entry.title,
        "author": entry.get("author", "unknown"),
        "published": entry.published,
        "link": entry.link,
        "summary": entry.summary
    })

df_medium = pd.DataFrame(medium_posts)
print("Medium rows:", len(df_medium))
df_medium.to_csv("medium_posts.csv", index=False)

final_df = pd.concat([df_reddit, df_news, df_hn, df_medium], ignore_index=True)
print("Final dataset rows:", len(final_df))
final_df.to_csv("final_dataset.csv", index=False)

import requests

API_KEY = "79482648b96b4d07861ce9e30558b391"   # make sure you keep your API key here

# define a new list for extra news articles
extra_articles = []

extra_queries = [
    "cloud computing", "natural language processing", "robotics",
    "cybersecurity", "fintech", "blockchain",
    "quantum computing", "edge AI", "data visualization", "neural networks"
]

for q in extra_queries:
    url = f"https://newsapi.org/v2/everything?q={q}&language=en&pageSize=100&apiKey={API_KEY}"
    response = requests.get(url).json()
    if "articles" in response:
        for a in response["articles"]:
            extra_articles.append({
                "source": "newsapi",
                "query": q,
                "title": a.get("title"),
                "author": a.get("author"),
                "description": a.get("description"),
                "publishedAt": a.get("publishedAt"),
                "url": a.get("url")
            })

# convert into DataFrame
df_news_extra = pd.DataFrame(extra_articles)
print("Extra news rows:", len(df_news_extra))

final_df = pd.concat([df_reddit, df_news, df_hn, df_medium, df_news_extra], ignore_index=True)
print("Final dataset rows:", len(final_df))

# Save the final file
final_df.to_csv("final_dataset.csv", index=False)

from google.colab import files
files.download("final_dataset.csv")